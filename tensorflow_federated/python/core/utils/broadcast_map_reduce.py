# Lint as: python2, python3
# Copyright 2019, The TensorFlow Federated Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""A representation for TensorFlow/MapReduce-based iterative processes."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from tensorflow_federated.proto.v0 import computation_pb2
from tensorflow_federated.python.common_libs import py_typecheck
from tensorflow_federated.python.core import api as tff

# TODO(b/134543154): While it seems impossible to find the class name that hits
# all the right checkboxes, we should probably iterate on this.


class BroadcastMapReduce(object):
  """A representation of a simple TensorFlow/round-based iterative process.

  This standardized representation can be used to describe a range of iterative
  processes that involve a single round of federated broadcast, map, and
  aggregation, as well as bookkeeping on the server side before and after, and
  where all local processing is done in TensorFlow. Computations represented in
  this form can be easily consumed by a range of backend systems that are able
  to orchestrate their execution in a MapReduce-like fashion. This can include
  systems that run static data pipelines such Apache Beam or Hadoop, but also
  notably, production platforms like the one described in the following paper:

  "Towards Federated Learning at Scale: System Design"
  https://arxiv.org/pdf/1902.01046.pdf

  Computations can be reduced into this form by the TFF compiler pipeline.

  In the description below, where it is unambiguous, we may also refer to this
  form as a "round-based" form, although it should be noted that not every
  computation that proceeds in synchronous rounds can be represented like this.

  We say that a `tff.utils.IterativeProcess` is *in the broadcast-map-reduce
  (or simply "round-based") form* if its iterative component (`next`) can be
  converted into a semantically equivalent instance of the computation template
  shown below (see `run_one_round`), with all the variable constituents of the
  template representable as simple sections of TensorFlow logic. A process in
  such form can then be equivalently represented by listing only the variable
  elements of the template (the tuple of TensorFlow computations that appear
  as parameters of federated operators). Such a compact representations can be
  encapsulated as an instance of this class.

  Cconceptually, `run_one_round`, as the iterator part (`next`) of an iterative
  process, is modeled in the same way as any stateful computation in TFF, i.e.,
  one that takes the server state as the first component of the input, and
  returns updated server state as the first component of the output. In addition
  to updating state, `run_one_round` additionally takes client-side data as
  input, and returns a combo of client-side and server-side statistics at the
  output (possibly as an empty tuple if not needed).

  The type signature of `run_one_round`, in the concise TFF type notation (as
  defined in TFF's `computation.proto`), is as follows:

  ```
  (<S@SERVER,{D}@CLIENTS> -> <S@SERVER,M@SERVER,{E}@CLIENTS>)
  ```

  The above type signature involves the following abstract types:

  * `S` is the type of the state that is persisted at the server between rounds
    of processing. It is the state that a server-side centralized coordinator
    would restore from a persisted checkpoint at the beginning of each round,
    and that it would eventually save back into a checkpoint at the end of the
    round for use in the subsequent iteration. The server state will typically
    include things like, e.g., model weights in federated training.

    NOTE: This is also the type of the output of the `initialize` that produces
    the server state to feed into the first round (see the constructor argument
    list below).

  * `D` represents the type of per-client units of data that serve as the input
    to the computation. In almost all cases, this would be a sequence type,
    i.e., a data set in TensorFlow's parlance, although strictly speaking, this
    does not have to always be the case.

  * `M` represents the type of metrics generated by the server after each round.

  * `E` represents the type of client-side outputs, emitted per round at the
    clients, and consumed locally at each client. In most cases, this additional
    client-side output would not be needed. These outpus are not intended to be
    aggregated in a centralized fashion (anything that would be aggregated must
    go through the federated aggregation operator described below).

  One can think of the process based on this representation as being equivalent
  to the following loop:

  ```
  data = ...
  state = initialize()
  while True:
    state, server_metrics, client_outputs = run_one_round(state, data)
  ```

  The logic of `run_one_round` in this form is defined as follows in terms of
  the 7 variable components `prepare`, `work`, `zero`, `accumulate`, `merge`,
  `report`, and `update` (in addition to `initialize` that produces the server
  state component for the initial round). The code below uses common syntactic
  shortcuts (such as federated zipping and unzipping) for brevity.

  ```
  def run_one_round(arg):
    s1 = arg[0]
    c1 = arg[1]
    s2 = tff.federated_apply(prepare, s1)
    c2 = federated_broadcast(s2)
    c3 = federated_map(work, [c1, c2])
    c4 = c3[0]
    c5 = c3[1]
    s3 = federated_aggregate(c4, zero, accumulate, merge, report)
    s4 = federated_apply(update, [s1, s3])
    s6 = s4[0]
    s7 = s4[1]
    return s6, s7, c5
  ```

  The above characterization of `run_one_round` depends on 7 pieces of a pure
  TensorFlow logic defined as follows. Please also consult the documentation
  for related federated operators (particularly `tff.federated_aggregate()`).

  * `prepare` represents the preparatory steps taken by the server to generate
    input that will be broadcast to the clients and that, together with the
    client data, will drive the client-side work in this round. It takes the
    initial state of the server, and produces the initial state for use by the
    clients. Its type signature is `(S -> C)`.

  * `work` represents the totality of client-side processing, again all as a
    single section of TensorFlow code. It takes a tuple of client data and
    initial state of the client that was broadcasted by the server, and returns
    a tuple consisting of an update to be aggregated (across clients) along
    with possibly local per-client outputs to be consumed locally by clients.
    Its type signature is `(<D,C> -> <U,E>)`. As noted earlier, the per-client
    local outputs will typically be absent. One example use of such outputs
    may involve, e.g., debugging metrics preserved locally (but not aggregated).

  * `zero` is the TensorFlow computation that produces the initial state of
    accumulators that are used to combine updates collected from subsets of the
    clients. In some systems, all accumulation may happen at the server, but
    for scalability reasons, it is often desirable to structure aggregation in
    tiers. Its type signature is `A`, or when represented as a
    `tff.Computation` in Python, `( -> A)`.

  * `accumulate` is the TensorFlow computation that updates the state of an
    update accumulator (initialized with `zero` above) with a single client's
    update. Its type signature is `(<A,U> -> A)`.

  * `merge` is the TensorFlow computation that merges two accumulators holding
    the results of aggregation over two disjoint subsets of clients. Its type
    signature is `(<A,A> -> A)`.

  * `report` is the TensorFlow computation that transforms the state of the
    top-most accumulator (after accumulating updates from all clients and
    merging all the resulting accumulators into a single one at the top level
    of the system hierarchy) into the final result of aggregation. Its type
    signature is `(A -> R)`.

  * `update` is the TensorFlow computation that applies the aggregate of all
    clients' updates (the output of `report`) to the server state to produce a
    new server state to feed into the next round, and that additionally outputs
    a set of server metrics to be reported externally as an output of this
    round. In federated learning scenarios, the metrics might include things
    like loss and accuracy, and the server state to be carried over may include
    the model weights to be trained further in a subsequent round. Its type
    signature is `(<S,R> -> <S,M>)`.

  The above TensorFlow computations' type signatures involves the following
  abstract types in addition to those defined earlier:

  * `C` is the type of the initial state of the clients, to be supplied by the
    server at the beginning of each round.

  * `U` is the type of the per-client update to be produced in each round and
    fed into the aggregation protocol.

  * `A` is the ype of the state of accumulators used to combine updates from
    subsets of clients.

  * `R` is the type of the final result of aggregating all client updates, to
    be incorporated into the server state at the end of the round.

  The individual TensorFlow computations that constitute an iterative process
  in this form are supplied as constructor arguments.
  """

  def __init__(self, initialize, prepare, work, zero, accumulate, merge, report,
               update):
    """Constructs a simple round-based representation of an iterative process.

    NOTE: All the computations supplied here as arguments must be TensorFlow
    computations, i.e., instances of `tff.Computation` constructed by the
    `tff.tf_computation` decorator/wrapper.

    Args:
      initialize: The computation that produces the initial server state.
      prepare: The computation that prepares the initial state for the clients.
      work: The client-side work computation.
      zero: The computation that produces the initial state for accumulators.
      accumulate: The computation that adds a client update to an accumulator.
      merge: The computation to use for merging pairs of accumulators.
      report: The computation that produces the final server-side aggregate for
        the top level accumulator.
      update: The computation that applies combined client update on the server
        and produces the server metrics.

    Raises:
      TypeError: If the Python or TFF types of the arguments are invalid or not
        compatible with each other.
      AssertionError: If the manner in which the given TensorFlow computations
        are represented by TFF does not match what this code is expecting (this
        is an internal error that requires code update).
    """
    for comp in [
        initialize, prepare, work, zero, accumulate, merge, report, update
    ]:
      py_typecheck.check_type(comp, tff.Computation)

      # TODO(b/130633916): Remove private access once an appropriate API for it
      # becomes available.
      comp_proto = comp._computation_proto  # pylint: disable=protected-access

      if not isinstance(comp_proto, computation_pb2.Computation):
        # Explicitly raised to force it to be done in non-debug mode as well.
        raise AssertionError('Could find the embedded computation definition.')
      which_comp = comp_proto.WhichOneof('computation')
      if which_comp != 'tensorflow':
        raise TypeError('Expected all computations supplied as arguments to '
                        'be TensorFlow, found {}.'.format(which_comp))

    if prepare.type_signature.parameter != initialize.type_signature.result:
      raise TypeError(
          'The `prepare` computation expects an argument of type {}, '
          'which does not match the result type {} of `initialize`.'.format(
              str(prepare.type_signature.parameter),
              str(initialize.type_signature.result)))

    if (not isinstance(work.type_signature.parameter, tff.NamedTupleType) or
        len(work.type_signature.parameter) != 2):
      raise TypeError(
          'The `work` computation expects an argument of type {} that is not '
          'a two-tuple.'.format(str(work.type_signature.parameter)))

    if work.type_signature.parameter[1] != prepare.type_signature.result:
      raise TypeError(
          'The `work` computation expects an argument tuple with type {} as '
          'the second element (the initial client state from the server), '
          'which does not match the result type {} of `prepare`.'.format(
              str(work.type_signature.parameter[1]),
              str(prepare.type_signature.result)))

    if (not isinstance(work.type_signature.result, tff.NamedTupleType) or
        len(work.type_signature.result) != 2):
      raise TypeError(
          'The `work` computation returns a result  of type {} that is not '
          'a two-tuple.'.format(str(work.type_signature.result)))

    expected_accumulate_type = tff.FunctionType(
        [zero.type_signature.result, work.type_signature.result[0]],
        zero.type_signature.result)
    if accumulate.type_signature != expected_accumulate_type:
      raise TypeError(
          'The `accumulate` computation has type signature {}, which does '
          'not match the expected {} as implied by the type signatures of '
          '`zero` and `work`.'.format(
              str(accumulate.type_signature), str(expected_accumulate_type)))

    expected_merge_type = tff.FunctionType(
        [accumulate.type_signature.result, accumulate.type_signature.result],
        accumulate.type_signature.result)
    if merge.type_signature != expected_merge_type:
      raise TypeError(
          'The `merge` computation has type signature {}, which does '
          'not match the expected {} as implied by the type signature '
          'of `accumulate`.'.format(
              str(merge.type_signature), str(expected_merge_type)))

    if report.type_signature.parameter != merge.type_signature.result:
      raise TypeError(
          'The `report` computation expects an argument of type {}, '
          'which does not match the result type {} of `merge`.'.format(
              str(report.type_signature.parameter),
              str(merge.type_signature.result)))

    expected_update_parameter_type = tff.to_type(
        [initialize.type_signature.result, report.type_signature.result])
    if update.type_signature.parameter != expected_update_parameter_type:
      raise TypeError(
          'The `update` computation expects an argument of type {}, '
          'which does not match the expected {} as implied by the type '
          'signatures of `initialize` and `report`.'.format(
              str(update.type_signature.parameter),
              str(expected_update_parameter_type)))

    if (not isinstance(update.type_signature.result, tff.NamedTupleType) or
        len(update.type_signature.result) != 2):
      raise TypeError(
          'The `update` computation returns a result  of type {} that is not '
          'a two-tuple.'.format(str(update.type_signature.result)))

    if update.type_signature.result[0] != initialize.type_signature.result:
      raise TypeError(
          'The `update` computation returns a result tuple with type {} as '
          'the first element (the updated state of the server), which does '
          'not match the result type {} of `initialize`.'.format(
              str(update.type_signature.result[0]),
              str(initialize.type_signature.result)))

    self._initialize = initialize
    self._prepare = prepare
    self._work = work
    self._zero = zero
    self._accumulate = accumulate
    self._merge = merge
    self._report = report
    self._update = update

  @property
  def initialize(self):
    return self._initialize

  @property
  def prepare(self):
    return self._prepare

  @property
  def work(self):
    return self._work

  @property
  def zero(self):
    return self._zero

  @property
  def accumulate(self):
    return self._accumulate

  @property
  def merge(self):
    return self._merge

  @property
  def report(self):
    return self._report

  @property
  def update(self):
    return self._update
